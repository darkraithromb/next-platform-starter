{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO549SIqMEfTQlg1Fkz9SeJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/darkraithromb/next-platform-starter/blob/main/Mpox.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IeghRCe-aAlT"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from keras.applications import Xception, DenseNet169\n",
        "from keras.layers import Concatenate, Dense, GlobalAveragePooling2D\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "# Set the seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Define the path to the dataset\n",
        "train_dir = 'path_to_your_train_directory'\n",
        "test_dir = 'path_to_your_test_directory'\n",
        "\n",
        "# Define the image dimensions\n",
        "img_height, img_width = 150, 150\n",
        "\n",
        "# Define the batch size\n",
        "batch_size = 16\n",
        "\n",
        "# Define the number of epochs\n",
        "epochs = 10\n",
        "\n",
        "# Define the learning rate\n",
        "learning_rate = 0.0001\n",
        "\n",
        "# Define the early stopping patience\n",
        "patience = 5\n",
        "\n",
        "# Define the reduce learning rate patience\n",
        "reduce_lr_patience = 3\n",
        "\n",
        "# Define the data augmentation parameters\n",
        "data_augmentation = {\n",
        "    'rotation_range': 50,\n",
        "    'width_shift_range': 0.2,\n",
        "    'height_shift_range': 0.2,\n",
        "    'shear_range': 0.25,\n",
        "    'zoom_range': 0.1,\n",
        "    'channel_shift_range': 20,\n",
        "    'horizontal_flip': True,\n",
        "    'vertical_flip': True,\n",
        "    'rescale': 1/255\n",
        "}\n",
        "\n",
        "# Create the data generators for training and testing\n",
        "train_datagen = ImageDataGenerator(**data_augmentation)\n",
        "test_datagen = ImageDataGenerator(rescale=1/255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Load the pre-trained Xception model\n",
        "xception_base = Xception(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n",
        "\n",
        "# Load the pre-trained DenseNet-169 model\n",
        "densenet_base = DenseNet169(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n",
        "\n",
        "# Freeze the base layers\n",
        "for layer in xception_base.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "for layer in densenet_base.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Define the ensemble model\n",
        "x = xception_base.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "\n",
        "y = densenet_base.output\n",
        "y = GlobalAveragePooling2D()(y)\n",
        "y = Dense(128, activation='relu')(y)\n",
        "\n",
        "z = Concatenate()([x, y])\n",
        "z = Dense(4, activation='softmax')(z)\n",
        "\n",
        "model = Model(inputs=[xception_base.input, densenet_base.input], outputs=z)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(lr=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define the early stopping and reduce learning rate callbacks\n",
        "early_stopping = EarlyStopping(patience=patience, min_delta=0.001)\n",
        "reduce_lr = ReduceLROnPlateau(patience=reduce_lr_patience, min_lr=0.00001)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=epochs,\n",
        "    validation_data=test_generator,\n",
        "    callbacks=[early_stopping, reduce_lr]\n",
        ")\n",
        "\n",
        "# Plot the training and validation accuracy and loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(test_generator)\n",
        "print(f'Test Loss: {test_loss:.3f}')\n",
        "print(f'Test Accuracy: {test_acc:.3f}')\n",
        "\n",
        "# Get the predictions on the test set\n",
        "predictions = model.predict(test_generator)\n",
        "\n",
        "# Convert the predictions to class labels\n",
        "predicted_classes = np.argmax(predictions, axis=1"
      ]
    }
  ]
}